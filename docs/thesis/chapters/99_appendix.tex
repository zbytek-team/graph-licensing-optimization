\chapter{Wybrane fragmenty implementacji}

\section{Organizacja skryptów eksperymentalnych}
Środowisko obliczeniowe zorganizowano jako spójny zestaw modułów
Pythona: pakiety do benchmarków statycznych, osobne moduły do
symulacji dynamicznych i rozszerzeń, a także proste komendy CLI do
szybkiej diagnostyki pojedynczych algorytmów. Wspólny rdzeń dba o
jednolite budowanie i walidację rozwiązań, a skrypty analityczne
tworzą raporty i wykresy użyte w częściach eksperymentalnych.

\section{Algorytmy dokładne}
\subsection{Program całkowitoliczbowy}
Poniżej zamieszczono fragment implementacji solvera ILP odpowiedzialnego
za wyznaczanie rozwiązań wzorcowych.

\begin{verbatim}
from collections.abc import Sequence
from typing import Any

import networkx as nx
import pulp

from glopt.core import (
    Algorithm, LicenseGroup, LicenseType, Solution
)

VAR_TRUE_THRESHOLD = 0.5


class ILPSolver(Algorithm):
    @property
    def name(self) -> str:
        return "ilp"

    def solve(
        self,
        graph: nx.Graph,
        license_types: Sequence[LicenseType],
        **kwargs: Any,
    ) -> Solution:
        time_limit: int | None = kwargs.get("time_limit")

        nodes: list[Any] = list(graph.nodes())
        model = pulp.LpProblem(
            "graph_licensing_optimization", pulp.LpMinimize
        )

        assign_vars: dict[tuple[Any, Any, int], pulp.LpVariable] = {}
        for i in nodes:
            neighborhood_i: set[Any] = set(graph.neighbors(i)) | {i}
            for j in neighborhood_i:
                for t_idx, _lt in enumerate(license_types):
                    assign_vars[i, j, t_idx] = pulp.LpVariable(
                        f"x_{i}_{j}_{t_idx}", cat="Binary"
                    )

        active_vars: dict[tuple[Any, int], pulp.LpVariable] = {}
        for i in nodes:
            for t_idx, _lt in enumerate(license_types):
                active_vars[i, t_idx] = pulp.LpVariable(
                    f"group_active_{i}_{t_idx}", cat="Binary"
                )

        model += pulp.lpSum(
            active_vars[i, t_idx] * lt.cost
            for i in nodes
            for t_idx, lt in enumerate(license_types)
        )

        for i in nodes:
            model += pulp.lpSum(
                active_vars[i, t_idx]
                for t_idx in range(len(license_types))
            ) <= 1

        for j in nodes:
            neighborhood_j: set[Any] = set(graph.neighbors(j)) | {j}
            model += pulp.lpSum(
                assign_vars.get((i, j, t_idx), 0)
                for i in neighborhood_j
                for t_idx in range(len(license_types))
            ) == 1

        for i in nodes:
            neighborhood_i = set(graph.neighbors(i)) | {i}
            for t_idx, lt in enumerate(license_types):
                group_size = pulp.lpSum(
                    assign_vars.get((i, j, t_idx), 0)
                    for j in neighborhood_i
                )
                model += group_size <= active_vars[i, t_idx] * \
                         lt.max_capacity
                model += group_size >= active_vars[i, t_idx] * \
                         lt.min_capacity

        for i in nodes:
            for t_idx, _lt in enumerate(license_types):
                var: Any | None = assign_vars.get((i, i, t_idx))
                if var is not None:
                    model += var >= active_vars[i, t_idx]

        solver = (
            pulp.PULP_CBC_CMD(msg=False, timeLimit=time_limit)
            if time_limit
            else pulp.PULP_CBC_CMD(msg=False)
        )
        model.solve(solver)

        try:
            self.last_status = pulp.LpStatus[model.status]
            obj_val = (
                pulp.value(model.objective)
                if model.objective is not None else None
            )
            if isinstance(obj_val, (int, float)):
                self.last_objective = float(obj_val)
            else:
                self.last_objective = float("nan")
        except Exception:
            self.last_status = "UNKNOWN"
            self.last_objective = float("nan")

        groups: list[LicenseGroup] = []
        for i in nodes:
            for t_idx, lt in enumerate(license_types):
                val = float(active_vars[i, t_idx].varValue or 0.0)
                if val > VAR_TRUE_THRESHOLD:
                    members: set[Any] = set()
                    for j in set(graph.neighbors(i)) | {i}:
                        var = assign_vars.get((i, j, t_idx))
                        if var and float(var.varValue or 0.0) > \
                           VAR_TRUE_THRESHOLD:
                            members.add(j)
                    if members:
                        groups.append(
                            LicenseGroup(
                                license_type=lt,
                                owner=i,
                                additional_members=frozenset(
                                    members - {i}
                                ),
                            ),
                        )

        return Solution(groups=tuple(groups))
\end{verbatim}

\section{Algorytmy metaheurystyczne}
\subsection{Optymalizacja mrówkowa}
Kod przedstawia główną pętlę algorytmu mrówkowego wraz z
procedurami konstrukcji rozwiązania.

\begin{verbatim}
from __future__ import annotations

import random
from typing import TYPE_CHECKING, Any, cast
from collections.abc import Sequence

from glopt.algorithms.greedy import GreedyAlgorithm
from glopt.core import (
    Algorithm, LicenseGroup, LicenseType, Solution
)
from glopt.core.solution_validator import SolutionValidator

if TYPE_CHECKING:
    import networkx as nx

PKey = tuple[Any, str]


class AntColonyOptimization(Algorithm):
    @property
    def name(self) -> str:
        return "ant_colony_optimization"

    def __init__(
        self,
        alpha: float = 1.0,
        beta: float = 2.0,
        evaporation: float = 0.5,
        q0: float = 0.9,
        num_ants: int = 20,
        max_iterations: int = 100,
    ) -> None:
        self.alpha = alpha
        self.beta = beta
        self.evap = evaporation
        self.q0 = q0
        self.num_ants = num_ants
        self.max_iter = max_iterations
        self.validator = SolutionValidator(debug=False)

    def solve(
        self,
        graph: nx.Graph,
        license_types: Sequence[LicenseType],
        **kwargs: Any
    ) -> Solution:
        seed = kwargs.get("seed")
        if isinstance(seed, int):
            random.seed(seed)
        deadline = kwargs.get("deadline")
        max_iter = int(kwargs.get("max_iterations", self.max_iter))
        num_ants = int(kwargs.get("num_ants", self.num_ants))
        initial: Solution | None = kwargs.get("initial_solution")

        pher = self._init_pher(graph, license_types)
        heur = self._init_heur(graph, license_types)

        if initial is not None and \
           self.validator.is_valid_solution(initial, graph):
            best = initial
        else:
            best = GreedyAlgorithm().solve(graph, license_types)
        ok, _ = self.validator.validate(best, graph)
        if not ok:
            best = self._fallback_singletons(graph, license_types)
        best_cost = best.total_cost
        self._deposit(pher, best)

        from time import perf_counter as _pc

        for _ in range(max_iter):
            if deadline is not None and _pc() >= float(deadline):
                break
            improved = False
            for _ in range(num_ants):
                cand = self._construct(
                    graph, license_types, pher, heur
                )
                ok, _ = self.validator.validate(cand, graph)
                if not ok:
                    continue
                if cand.total_cost < best_cost:
                    best, best_cost, improved = (
                        cand, cand.total_cost, True
                    )
            self._evaporate(pher)
            self._deposit(pher, best)
            if not improved:
                continue
        ok, _ = self.validator.validate(best, graph)
        if not ok:
            return self._fallback_singletons(graph, license_types)
        return best

    def _construct(
        self,
        graph: nx.Graph,
        lts: Sequence[LicenseType],
        pher: dict[PKey, float],
        heur: dict[PKey, float],
    ) -> Solution:
        uncovered: set[Any] = set(graph.nodes())
        groups: list[LicenseGroup] = []
        while uncovered:
            owner = self._select_owner(uncovered, lts, pher, heur)
            owner = owner if owner is not None else next(iter(uncovered))
            lt = self._select_license(owner, lts, pher, heur) or \
                 min(lts, key=lambda x: x.cost)

            pool = (set(graph.neighbors(owner)) | {owner}) & uncovered
            if len(pool) < lt.min_capacity:
                singles = [
                    x for x in lts if x.min_capacity <= 1 <= x.max_capacity
                ]
                if singles:
                    lt1 = min(singles, key=lambda x: x.cost)
                    groups.append(
                        LicenseGroup(lt1, owner, frozenset())
                    )
                    uncovered.remove(owner)
                else:
                    lt1 = min(lts, key=lambda x: x.cost)
                    groups.append(
                        LicenseGroup(lt1, owner, frozenset())
                    )
                    uncovered.remove(owner)
                continue

            k = max(0, lt.max_capacity - 1)
            degv = cast(Any, graph.degree)

            def _deg_val(node: Any) -> int:
                return int(degv[node])

            add = sorted(
                (pool - {owner}), key=_deg_val, reverse=True
            )[:k]
            groups.append(
                LicenseGroup(lt, owner, frozenset(add))
            )
            uncovered -= {owner} | set(add)
        return Solution(groups=tuple(groups))

    def _select_owner(
        self,
        uncovered: set[Any],
        lts: Sequence[LicenseType],
        pher: dict[PKey, float],
        heur: dict[PKey, float],
    ) -> Any | None:
        if not uncovered:
            return None
        scores: dict[Any, float] = {}
        for n in uncovered:
            acc = 0.0
            for lt in lts:
                tau = pher.get((n, lt.name), 1.0)
                eta = heur.get((n, lt.name), 1.0)
                acc += (tau**self.alpha) * (eta**self.beta)
            scores[n] = acc / max(1, len(lts))
        return self._roulette_or_best(list(uncovered), scores)

    def _select_license(
        self,
        owner: Any,
        lts: Sequence[LicenseType],
        pher: dict[PKey, float],
        heur: dict[PKey, float]
    ) -> LicenseType | None:
        if not lts:
            return None
        scores = {
            lt: (pher.get((owner, lt.name), 1.0) ** self.alpha) *
                (heur.get((owner, lt.name), 1.0) ** self.beta)
            for lt in lts
        }
        return self._roulette_or_best(lts, scores)

    def _roulette_or_best(
        self, choices: list[Any], scores: dict[Any, float]
    ) -> Any:
        if not choices:
            return None
        if random.random() < self.q0:
            return max(choices, key=lambda c: scores.get(c, 0.0))
        total = sum(max(0.0, scores.get(c, 0.0)) for c in choices)
        if total <= 0:
            return random.choice(choices)
        r = random.uniform(0, total)
        acc = 0.0
        for c in choices:
            acc += max(0.0, scores.get(c, 0.0))
            if acc >= r:
                return c
        return random.choice(choices)

    def _init_pher(
        self, graph: nx.Graph, lts: Sequence[LicenseType]
    ) -> dict[PKey, float]:
        return {(n, lt.name): 1.0 for n in graph.nodes() for lt in lts}

    def _init_heur(
        self, graph: nx.Graph, lts: Sequence[LicenseType]
    ) -> dict[PKey, float]:
        h: dict[PKey, float] = {}
        degv = cast(Any, graph.degree)
        for n in graph.nodes():
            deg = int(degv[n])
            for lt in lts:
                cap_eff = (lt.max_capacity / lt.cost) if lt.cost > 0 \
                          else 1e9
                h[n, lt.name] = cap_eff * (1.0 + float(deg))
        return h

    def _evaporate(self, pher: dict[PKey, float]) -> None:
        f = max(0.0, min(1.0, self.evap))
        for k in pher:
            pher[k] *= 1.0 - f

    def _deposit(self, pher: dict[PKey, float], sol: Solution) -> None:
        if sol.total_cost <= 0:
            return
        q = 1.0 / sol.total_cost
        for g in sol.groups:
            for n in g.all_members:
                k = (n, g.license_type.name)
                if k in pher:
                    pher[k] += q

    def _fallback_singletons(
        self, graph: nx.Graph, lts: Sequence[LicenseType]
    ) -> Solution:
        lt1 = min(
            [x for x in lts if x.min_capacity <= 1] or lts,
            key=lambda x: x.cost
        )
        groups = [
            LicenseGroup(lt1, n, frozenset()) for n in graph.nodes()
        ]
        return Solution(groups=tuple(groups))
\end{verbatim}

\section{Funkcje pomocnicze}
\subsection{Budowanie i scalanie grup}
Wybrane metody klasy budującej rozwiązania, wykorzystywane przez
algorytmy konstrukcyjne i metaheurystyki.

\begin{verbatim}
from collections.abc import Hashable, Sequence

import networkx as nx

from .models import LicenseGroup, LicenseType, Solution

N = Hashable


class SolutionBuilder:
    @staticmethod
    def create_solution_from_groups(
        groups: list[LicenseGroup]
    ) -> Solution:
        return Solution(groups=tuple(groups))

    @staticmethod
    def get_compatible_license_types(
        group_size: int,
        license_types: Sequence[LicenseType],
        exclude: LicenseType | None = None,
    ) -> list[LicenseType]:
        out: list[LicenseType] = []
        for lt in license_types:
            if exclude and lt == exclude:
                continue
            if lt.min_capacity <= group_size <= lt.max_capacity:
                out.append(lt)
        return out

    @staticmethod
    def get_owner_neighbors_with_self(
        graph: nx.Graph, owner: N
    ) -> set[N]:
        return set(graph.neighbors(owner)) | {owner}

    @staticmethod
    def merge_groups(
        group1: LicenseGroup,
        group2: LicenseGroup,
        graph: nx.Graph,
        license_types: Sequence[LicenseType],
    ) -> LicenseGroup | None:
        members = group1.all_members | group2.all_members
        size = len(members)

        for lt in license_types:
            if lt.min_capacity <= size <= lt.max_capacity:
                for owner in members:
                    neigh = SolutionBuilder.get_owner_neighbors_with_self(
                        graph, owner
                    )
                    if members.issubset(neigh):
                        return LicenseGroup(
                            lt, owner, frozenset(members - {owner})
                        )
        return None

    @staticmethod
    def find_cheapest_single_license(
        license_types: Sequence[LicenseType]
    ) -> LicenseType:
        singles = [lt for lt in license_types if lt.min_capacity <= 1]
        return min(singles or list(license_types), key=lambda lt: lt.cost)

    @staticmethod
    def find_cheapest_license_for_size(
        size: int, license_types: Sequence[LicenseType]
    ) -> LicenseType | None:
        compat = [
            lt for lt in license_types
            if lt.min_capacity <= size <= lt.max_capacity
        ]
        return min(compat, key=lambda lt: lt.cost) if compat else None
\end{verbatim}

\section{Analiza wyników}
\subsection{Główny skrypt raportujący}
Poniższy fragment prezentuje funkcję główną narzędzia analitycznego,
które agreguje wyniki benchmarków i generuje raporty tekstowe oraz
graficzne.

\begin{verbatim}
def main() -> None:
    apply_plot_style()

    duolingo_raw = expand_license_counts(
        load_dataset(DATA_DIR / "duolingo_df.csv")
    )
    duolingo, duolingo_unit_costs = normalize_cost_columns(
        duolingo_raw, attach_group_multiplier=True
    )
    duolingo = apply_algorithm_labels(duolingo)
    duolingo["graph_key"] = duolingo["graph"]
    duolingo["graph"] = duolingo["graph_key"].map(GRAPH_LABELS) \
        .fillna(duolingo["graph_key"])
    duolingo["dataset"] = DATASET_LABELS["duolingo_super"]

    roman_raw = expand_license_counts(
        load_dataset(DATA_DIR / "roman_df.csv")
    )
    roman, roman_unit_costs = normalize_cost_columns(
        roman_raw, attach_group_multiplier=True
    )
    roman = apply_algorithm_labels(roman)
    roman["graph_key"] = roman["graph"]
    roman["graph"] = roman["graph_key"].map(GRAPH_LABELS) \
        .fillna(roman["graph_key"])
    roman["dataset"] = DATASET_LABELS["roman_domination"]

    timeout_raw = expand_license_counts(
        load_dataset(DATA_DIR / "timeout_df.csv")
    )
    combined_unit_costs = {**duolingo_unit_costs, **roman_unit_costs}
    timeout, _ = normalize_cost_columns(
        timeout_raw,
        unit_costs=combined_unit_costs,
        attach_group_multiplier=True,
    )
    timeout = apply_algorithm_labels(timeout)
    timeout["graph_key"] = timeout["graph"]
    timeout["graph"] = timeout["graph_key"].map(GRAPH_LABELS) \
        .fillna(timeout["graph_key"])

    combined = pd.concat([duolingo, roman], ignore_index=True)

    numeric_cols = [
        "time_s", "total_cost", "cost_per_node",
        "license_group", "license_individual",
    ]
    if "license_group_multiplier" in duolingo.columns:
        numeric_cols.append("license_group_multiplier")

    duo_overall = describe_numeric(duolingo, numeric_cols)
    save_table(duo_overall, TAB_DIR / "duolingo_overall_stats.csv")

    duo_algo = describe_numeric(duolingo, numeric_cols, ["algorithm"])
    save_table(duo_algo, TAB_DIR / "duolingo_algorithm_stats.csv")

    duo_graph = describe_numeric(
        duolingo, ["time_s", "total_cost", "cost_per_node"],
        ["algorithm", "graph"]
    )
    save_table(duo_graph, TAB_DIR / "duolingo_algorithm_graph_stats.csv")

    roman_overall = describe_numeric(roman, numeric_cols)
    save_table(roman_overall, TAB_DIR / "roman_overall_stats.csv")

    roman_algo = describe_numeric(roman, numeric_cols, ["algorithm"])
    save_table(roman_algo, TAB_DIR / "roman_algorithm_stats.csv")

    roman_graph = describe_numeric(
        roman, ["time_s", "total_cost", "cost_per_node"],
        ["algorithm", "graph"]
    )
    save_table(roman_graph, TAB_DIR / "roman_algorithm_graph_stats.csv")

    timeout_counts_algo = (
        timeout.groupby("algorithm")
        .size().rename("count")
        .sort_values(ascending=False)
    )
    timeout_counts_algo.to_csv(TAB_DIR / "timeouts_by_algorithm.csv")

    timeout_counts_graph = (
        timeout.groupby(["algorithm", "graph"]).size().rename("count")
    )
    timeout_counts_graph.to_csv(
        TAB_DIR / "timeouts_by_algorithm_graph.csv"
    )

    timeout_license = (
        timeout.groupby(["algorithm", "license_config"])
        .size().rename("count")
    )
    timeout_license.to_csv(
        TAB_DIR / "timeouts_by_algorithm_license.csv"
    )

    pareto_cols = [
        "algorithm", "graph", "n_nodes",
        "total_cost", "time_s", "cost_per_node",
    ]
    pareto_df = compute_pareto_front(
        duolingo[pareto_cols], "total_cost", "time_s"
    )
    pareto_df.to_csv(
        TAB_DIR / "duolingo_pareto_cost_time.csv", index=False
    )

    id_cols = ["graph", "n_nodes", "license_config", "rep", "sample"]
    pivot_time = pivot_complete_blocks(
        duolingo, id_cols, "algorithm", "time_s"
    )
    pivot_cost = pivot_complete_blocks(
        duolingo, id_cols, "algorithm", "cost_per_node"
    )

    friedman_reports: list[str] = []
    time_result = run_friedman_nemenyi(pivot_time)
    if time_result:
        save_table(
            time_result.mean_ranks.to_frame("mean_rank"),
            TAB_DIR / "friedman_time_mean_ranks.csv",
        )
        if time_result.nemenyi is not None:
            save_table(
                time_result.nemenyi,
                TAB_DIR / "nemenyi_time_pvalues.csv",
            )
        friedman_reports.append(
            f"Friedman test (time_s): "
            f"statistic={time_result.statistic:.3f}, "
            f"p-value={time_result.pvalue:.4g}"
        )
    else:
        friedman_reports.append(
            "Friedman test (time_s): insufficient paired samples"
        )

    cost_result = run_friedman_nemenyi(pivot_cost)
    if cost_result:
        save_table(
            cost_result.mean_ranks.to_frame("mean_rank"),
            TAB_DIR / "friedman_cost_per_node_mean_ranks.csv",
        )
        if cost_result.nemenyi is not None:
            save_table(
                cost_result.nemenyi,
                TAB_DIR / "nemenyi_cost_per_node_pvalues.csv",
            )
        friedman_reports.append(
            f"Friedman test (cost_per_node): "
            f"statistic={cost_result.statistic:.3f}, "
            f"p-value={cost_result.pvalue:.4g}"
        )
    else:
        friedman_reports.append(
            "Friedman test (cost_per_node): insufficient paired samples"
        )

    _plot_metric_vs_nodes(
        duolingo, "time_s", "Duolingo: czas vs liczba węzłów",
        "duolingo_time_vs_nodes.pdf"
    )
    _plot_metric_vs_nodes(
        duolingo, "total_cost", "Duolingo: koszt vs liczba węzłów",
        "duolingo_cost_vs_nodes.pdf"
    )
    _plot_metric_vs_nodes(
        duolingo, "cost_per_node",
        "Duolingo: koszt/węzeł vs liczba węzłów",
        "duolingo_cost_per_node_vs_nodes.pdf"
    )
    _plot_metric_by_graph(
        duolingo, "time_s", "Duolingo: czasy według typu grafu",
        "duolingo_time_by_graph", ymax=15.0
    )
    _plot_metric_by_graph(
        duolingo, "cost_per_node",
        "Duolingo: koszt/węzeł według grafu",
        "duolingo_cost_per_node_by_graph"
    )

    pareto_fig, ax = plt.subplots()
    pareto_algos = duolingo["algorithm"].unique().tolist()
    sns.scatterplot(
        data=duolingo, x="total_cost", y="time_s",
        hue="algorithm",
        palette=algorithm_palette(pareto_algos),
        alpha=0.4, ax=ax,
    )
    sns.lineplot(
        data=pareto_df, x="total_cost", y="time_s",
        color="black", label="Pareto", ax=ax
    )
    ax.set_title("Duolingo: Pareto koszt-czas")
    ax.set_xlabel("Łączny koszt")
    ax.set_ylabel("Czas [s]")
    ax.set_ylim(bottom=0)
    handles, labels = ax.get_legend_handles_labels()
    if handles:
        ax.legend(handles, labels, title="Algorytm")
    plt.tight_layout()
    pareto_fig.savefig(FIG_DIR / "duolingo_pareto_cost_time.pdf")
    plt.close(pareto_fig)

    duo_license_totals = duolingo[
        ["license_group", "license_individual", "license_other"]
    ].sum()
    roman_license_totals = roman[
        ["license_group", "license_individual", "license_other"]
    ].sum()
    license_summary = pd.DataFrame(
        {
            "dataset": [
                DATASET_LABELS["duolingo_super"],
                DATASET_LABELS["roman_domination"],
            ],
            "group": [
                duo_license_totals["license_group"],
                roman_license_totals["license_group"],
            ],
            "individual": [
                duo_license_totals["license_individual"],
                roman_license_totals["license_individual"],
            ],
            "other": [
                duo_license_totals["license_other"],
                roman_license_totals["license_other"],
            ],
        }
    )
    for column in ["group", "individual", "other"]:
        license_summary[column] = (
            license_summary[column].round().astype(int)
        )
    license_summary["total"] = license_summary[
        ["group", "individual", "other"]
    ].sum(axis=1)
    license_summary["group_share"] = (
        license_summary["group"] / license_summary["total"]
    )
    license_summary["individual_share"] = (
        license_summary["individual"] / license_summary["total"]
    )
    save_table(license_summary, TAB_DIR / "license_mix_duo_vs_roman.csv")
    _plot_license_mix(license_summary, "license_mix_duo_vs_roman.pdf")

    license_summary_words = license_summary[
        ["dataset", "group", "individual", "other"]
    ].copy()
    license_summary_words["group_slowo"] = (
        license_summary_words["group"].apply(number_to_polish_words)
    )
    license_summary_words["individual_slowo"] = (
        license_summary_words["individual"].apply(number_to_polish_words)
    )
    license_summary_words["other_slowo"] = (
        license_summary_words["other"].apply(number_to_polish_words)
    )
    save_table(
        license_summary_words,
        TAB_DIR / "license_mix_duo_vs_roman_words.csv"
    )
    license_text_lines = [""]
    for _, row in license_summary_words.iterrows():
        license_text_lines.append(
            f"- {row['dataset']}: {row['group_slowo']} grupowych, "
            f"{row['individual_slowo']} indywidualnych, "
            f"{row['other_slowo']} pozostałych"
        )
    write_text(
        REPORTS_DIR / "license_mix_duo_vs_roman.md",
        license_text_lines
    )

    comparison = (
        combined.groupby(["dataset", "algorithm"])
        .agg(
            time_mean=("time_s", "mean"),
            cost_mean=("total_cost", "mean"),
            cpn_mean=("cost_per_node", "mean"),
        ).reset_index()
    )
    save_table(
        comparison, TAB_DIR / "duo_vs_roman_algorithm_means.csv"
    )

    by_graph = (
        combined.groupby(["dataset", "graph"])
        .agg(
            time_mean=("time_s", "mean"),
            cpn_mean=("cost_per_node", "mean"),
        ).reset_index()
    )
    save_table(
        by_graph, TAB_DIR / "duo_vs_roman_graph_means.csv"
    )
    _plot_duo_vs_roman(
        combined, "time_s", "Porównanie czasów według grafu",
        "duo_vs_roman_time_by_graph.pdf"
    )
    _plot_duo_vs_roman(
        combined, "cost_per_node",
        "Porównanie kosztu/węzeł według grafu",
        "duo_vs_roman_cost_per_node_by_graph.pdf"
    )

    lines: list[str] = []
    lines.append("")
    lines.append("")
    duo_label = DATASET_LABELS["duolingo_super"]
    roman_label = DATASET_LABELS["roman_domination"]
    lines.append(
        f"{duo_label} – średni czas: "
        f"{duo_overall.loc[duo_overall['metric'] == 'time_s', "
        f"'mean'].iloc[0]:.2f} s"
    )
    lines.append(
        f"{duo_label} – średni koszt/węzeł: "
        f"{duo_overall.loc[duo_overall['metric'] == 'cost_per_node', "
        f"'mean'].iloc[0]:.3f}"
    )
    lines.append(
        f"{roman_label} – średni czas: "
        f"{roman_overall.loc[roman_overall['metric'] == 'time_s', "
        f"'mean'].iloc[0]:.2f} s"
    )
    lines.append(
        f"{roman_label} – średni koszt/węzeł: "
        f"{roman_overall.loc[roman_overall['metric'] == 'cost_per_node', "
        f"'mean'].iloc[0]:.3f}"
    )
    lines.append("")
    lines.extend(friedman_reports)
    lines.append("")
    lines.append(
        f"Największe rozpiętości średniego czasu ({duo_label}):"
    )
    time_gap = _mean_gap(duolingo, "time_s", "graph")["range"].head(5)
    for alg, value in time_gap.items():
        lines.append(f"- {alg}: {value:.2f} s")
    lines.append("")
    lines.append(
        "Rozkład udziału licencji: license_mix_duo_vs_roman.csv "
        "oraz license_mix_duo_vs_roman_words.csv"
    )
    write_text(REPORTS_DIR / "summary.md", lines)


if __name__ == "__main__":
    main()
\end{verbatim}
